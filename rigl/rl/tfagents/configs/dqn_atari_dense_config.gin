# Configs to run DQN training for dense on atari environments.

train_eval.env_name='Pong'
# v4 aligns the action space with the Deepmind paper.
# Deterministic ensures that a fixed frameskip of 4 is applied.
train_eval.mode='Deterministic'
train_eval.version='v4'
# Use these for a Pong test vs the TFAgents dahsboard which used Pong-v0
# instead of PongDeterministic-v4
# train_eval.mode=''
# train_eval.version='v0'

train_eval.update_frequency = 4
train_eval.initial_collect_steps = 50000  # 50k
train_eval.num_iterations = 10000000  # 10m
train_eval.max_episode_frames_collect = 50000
train_eval.max_episode_frames_eval = 108000

train_eval.epsilon_greedy = 0.1
train_eval.epsilon_decay_period = 250000
train_eval.batch_size = 32
train_eval.learning_rate = 0.00025
train_eval.n_step_update = 1
train_eval.gamma = 0.99
train_eval.target_update_tau = 1.0
train_eval.target_update_period = 2500
train_eval.reward_scale_factor = 1.0

train_eval.replay_capacity = 1000000  # 1M
train_eval.policy_save_interval = 500000
train_eval.eval_interval = 1000
train_eval.eval_episodes = 30
train_eval.debug_summaries = False

train_eval.weight_decay = 0.0
train_eval.width = 1.0

train_eval.sparse_output_layer = False
train_eval.train_mode = 'dense'

mask_updater.update_alg = ''
mask_updater.schedule_alg = 'cosine'
log_snr.freq=5000
