{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5O1UdsY202_"
      },
      "source": [
        "##### Copyright 2020 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUW1g2_jWmBk"
      },
      "source": [
        "## Measuring Signal Properties of Various Initializations\n",
        "For a random signal x ~ normal(0, 1), and a neural network denoted with f(x)=y; ensuring std(y)=1 at initialization is a common goal for popular NN initialization schemes. Here we measure signal propagation for different sparse initializations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4rvDSX8FFYTI"
      },
      "outputs": [],
      "source": [
        "#@title Imports and Definitions\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "import gin\n",
        "from rigl import sparse_utils\n",
        "from rigl.rigl_tf2 import init_utils\n",
        "from rigl.rigl_tf2 import utils\n",
        "from rigl.rigl_tf2 import train\n",
        "from rigl.rigl_tf2 import networks\n",
        "from rigl.rigl_tf2 import mask_updaters\n",
        "\n",
        "import functools\n",
        "\n",
        "pruning_params = utils.get_pruning_params(mode='constant', final_sparsity = 0., begin_step=int(1e10))\n",
        "INPUT_SHAPE = (28, 28, 3)\n",
        "class Lenet5(tf.keras.Model):\n",
        "\n",
        "  def __init__(self,\n",
        "               input_shape,\n",
        "               num_classes,\n",
        "               activation: str,\n",
        "               hidden_sizes = (6, 16, 120, 84)):\n",
        "    super(Lenet5, self).__init__()\n",
        "    l = tf.keras.layers\n",
        "    kwargs = {'activation': activation}\n",
        "    filter_fn = lambda _: True\n",
        "    wrap_fn = functools.partial(utils.maybe_prune_layer, params=pruning_params, filter_fn=filter_fn)\n",
        "    self.conv1 =  wrap_fn(l.Conv2D(hidden_sizes[0], 5, input_shape=input_shape, **kwargs))\n",
        "    self.pool1 = l.MaxPool2D(pool_size=(2, 2))\n",
        "    self.conv2 =  wrap_fn(l.Conv2D(hidden_sizes[1], 5, input_shape=input_shape, **kwargs))\n",
        "    self.pool2 = l.MaxPool2D(pool_size=(2, 2))\n",
        "    self.flatten = l.Flatten()\n",
        "    self.dense1 = wrap_fn(l.Dense(hidden_sizes[2], **kwargs))\n",
        "    self.dense2 = wrap_fn(l.Dense(hidden_sizes[3], **kwargs))\n",
        "    self.dense3 = wrap_fn(l.Dense(num_classes, **kwargs))\n",
        "    self.build((1,)+input_shape)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = inputs\n",
        "    results = {}\n",
        "    for l_name in ['conv1', 'pool1', 'conv2', 'pool2', 'flatten', 'dense1', 'dense2', 'dense3']:\n",
        "      x = getattr(self, l_name)(x)\n",
        "      results[l_name] = x \n",
        "    return results\n",
        "\n",
        "def get_mask_random_numpy(mask_shape, sparsity):\n",
        "  \"\"\"Creates a random sparse mask with deterministic sparsity.\n",
        "\n",
        "  Args:\n",
        "    mask_shape: list, used to obtain shape of the random mask.\n",
        "    sparsity: float, between 0 and 1.\n",
        "\n",
        "  Returns:\n",
        "    numpy.ndarray\n",
        "  \"\"\"\n",
        "  all_ones = np.abs(np.ones(mask_shape))\n",
        "  n_zeros = int(np.floor(sparsity * all_ones.size))\n",
        "  rand_vals = np.random.uniform(size=mask_shape, high=range(1,mask_shape[-1]+1))\n",
        "  randflat=rand_vals.flatten()\n",
        "  randflat.sort()\n",
        "  t = randflat[n_zeros]\n",
        "  all_ones[rand_vals\u003c=t] = 0\n",
        "  return all_ones\n",
        "\n",
        "def create_convnet(sparsity=0, weight_init_method = None, scale=2, method='fanin_normal'):\n",
        "  model = Lenet5(INPUT_SHAPE, num_classes, 'relu')\n",
        "  if sparsity \u003e 0:\n",
        "    all_masks = [layer.pruning_vars[0][1] for layer in model.layers if isinstance(layer, utils.PRUNING_WRAPPER)]\n",
        "    for mask in all_masks:\n",
        "      new_mask = tf.cast(get_mask_random_numpy(mask.shape, sparsity), dtype=mask.dtype)\n",
        "      mask.assign(new_mask)\n",
        "    if weight_init_method:\n",
        "      all_weights = [layer.pruning_vars[0][0] for layer in model.layers if isinstance(layer, utils.PRUNING_WRAPPER)]\n",
        "      for mask, param in zip(all_masks, all_weights):\n",
        "        if weight_init_method == 'unit':\n",
        "          new_init = init_utils.unit_scaled_init(mask, method=method, scale=scale)\n",
        "        elif weight_init_method == 'layer':\n",
        "          new_init = init_utils.layer_scaled_init(mask, method=method, scale=scale)\n",
        "        else:\n",
        "          raise ValueError\n",
        "        param.assign(new_init)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkZ_GNjyYYqZ"
      },
      "source": [
        "Here we demonstrate how we can calculate the standard deviation of random noise at initialization for `layer-wise` scaled initialization of Liu et. al."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsmPRCuZnxDA"
      },
      "outputs": [],
      "source": [
        "# Let's create a 95% sparse Lenet-5.\n",
        "model = create_convnet(sparsity=0.95, weight_init_method='layer', scale=2, method='fanin_normal')\n",
        "# Random input signal\n",
        "random_input = tf.random.normal((1000,) + INPUT_SHAPE)\n",
        "output_dict = model(random_input)\n",
        "all_stds = []\n",
        "for k in ['dense1', 'dense2', 'dense3']:\n",
        "  out_dim = output_dict[k].shape[-1]\n",
        "  stds = np.std(np.reshape(output_dict[k], (-1,out_dim)),axis=0)\n",
        "  all_stds.append(stds)\n",
        "print('Mean deviation per neuron', np.mean(np.concatenate(all_stds, axis=0)))\n",
        "print('Mean deviation per output neuron', np.mean(all_stds[-1]))\n",
        "print('Deviation at output', np.std(random_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3ttY88rYovo"
      },
      "source": [
        "Now we define the code above as a function and use it on a grid to plot signal propagation at different sparsities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "executionInfo": {
          "elapsed": 320,
          "status": "ok",
          "timestamp": 1613388807790,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": -180
        },
        "id": "4rfMGKciOOHf"
      },
      "outputs": [],
      "source": [
        "def propagate_signal(sparsity, init_method, batch_size=500):\n",
        "  model = create_convnet(sparsity=sparsity, weight_init_method=init_method)\n",
        "  random_input = tf.random.normal((batch_size,) + INPUT_SHAPE)\n",
        "  # print(np.mean(random_input), np.std(random_input))\n",
        "  output_dict = model(random_input)\n",
        "  out_std = np.std(output_dict['dense3'])\n",
        "  all_stds = []\n",
        "  for k in ['dense1', 'dense2', 'dense3']:\n",
        "    out_dim = output_dict[k].shape[-1]\n",
        "    stds = np.std(np.reshape(output_dict[k], (-1,out_dim)),axis=0)\n",
        "    all_stds.append(stds)\n",
        "  meanstd = np.mean(np.concatenate(all_stds, axis=0))\n",
        "  return meanstd, out_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1rNPLXk7Ins"
      },
      "outputs": [],
      "source": [
        "import itertools, collections\n",
        "import numpy as np\n",
        "all_results = collections.defaultdict(dict)\n",
        "\n",
        "N_EXP = 3\n",
        "for s in np.linspace(0.8,0.98,5):\n",
        "  print(s)\n",
        "  for  method, name in zip((None, 'unit', 'layer'), ('Masked Dense', 'Ours', 'Scaled-Init')):\n",
        "    all_results[name][s] = [propagate_signal(s, method) for _ in range(N_EXP)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sbjc7LxpVGl0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for k, v in all_results.items():\n",
        "  # if k == 'Masked Dense':\n",
        "  #   continue\n",
        "  x = sorted(v.keys())\n",
        "  y = [np.mean([vv[1] for vv in v[kk]])+1e-5 for kk in x]\n",
        "  plt.plot(x, y, label=k)\n",
        "plt.hlines(y=1, color='r', xmin=0, xmax=1)\n",
        "plt.yscale('log')\n",
        "plt.title('std(output)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "for k, v in all_results.items():\n",
        "  # if k == 'Masked Dense':\n",
        "  #   continue\n",
        "  x = sorted(v.keys())\n",
        "  y = [np.mean([vv[0] for vv in v[kk]])+1e-5 for kk in x]\n",
        "  plt.plot(x, y, label=k)\n",
        "plt.yscale('log')\n",
        "plt.hlines(y=1, color='r', xmin=0, xmax=1)\n",
        "plt.title('mean(std_per_neuron)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "Mnist propagation init sparse .ipynb",
      "provenance": [
        {
          "file_id": "126QJDydlS0V4tQ-KhiN6bSlCOisqLV-Z",
          "timestamp": 1612472405306
        },
        {
          "file_id": "137QdNeUdTGoAOEPKpPMC09keiwlu12Bh",
          "timestamp": 1601472560303
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
