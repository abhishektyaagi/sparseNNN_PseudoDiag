{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e5O1UdsY202_"
      },
      "source": [
        "##### Copyright 2020 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5p1fkA3rgL_"
      },
      "outputs": [],
      "source": [
        "# Download the official ResNet50 implementation and other libraries.\n",
        "# the ResNet50 module s.t. we can use the model builders for our counting.\n",
        "%%bash \n",
        "test -d tpu || git clone https://github.com/tensorflow/tpu tpu \u0026\u0026 mv tpu/models/experimental/resnet50_keras ./ \n",
        "test -d rigl || git clone https://github.com/google-research/rigl rigl_repo \u0026\u0026 mv rigl_repo/rigl ./ \n",
        "test -d gresearch || git clone https://github.com/google-research/google-research google_research"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmr3djWe1rKj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from micronet_challenge import counting\n",
        "from resnet50_keras import resnet_model as resnet_keras\n",
        "from rigl import sparse_utils\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYm9k-Q47PXe"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "model = resnet_keras.ResNet50(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNS1s5Wm7U8-"
      },
      "outputs": [],
      "source": [
        "masked_layers = []\n",
        "for layer in model.layers:\n",
        "  if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
        "    masked_layers.append(layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtD03TrBSDzV"
      },
      "outputs": [],
      "source": [
        "PARAM_SIZE=32 # bits\n",
        "import functools\n",
        "get_stats = functools.partial(\n",
        "    sparse_utils.get_stats, first_layer_name='conv1', last_layer_name='fc1000',\n",
        "    param_size=PARAM_SIZE)\n",
        "def print_stats(masked_layers, default_sparsity=0.8, method='erdos_renyi',\n",
        "                custom_sparsities={}, is_debug=False, width=1., **kwargs):\n",
        "  print('Method: %s, Sparsity: %f' % (method, default_sparsity))\n",
        "  total_flops, total_param_bits, sparsity = get_stats(\n",
        "      masked_layers, default_sparsity=default_sparsity, method=method,\n",
        "      custom_sparsities=custom_sparsities, is_debug=is_debug, width=width, **kwargs)\n",
        "  print('Total Flops: %.3f MFlops' % (total_flops/1e6))\n",
        "  print('Total Size: %.3f Mbytes' % (total_param_bits/8e6))\n",
        "  print('Real Sparsity: %.3f' % (sparsity))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_2kH9dsrUqu"
      },
      "source": [
        "# Pruning FLOPs\n",
        "We calculate theoratical FLOPs for pruning, which means we will start counting sparse FLOPs when the pruning starts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHmbXdMyT2c8"
      },
      "outputs": [],
      "source": [
        "p_start, p_end, p_freq = 10000,25000,1000\n",
        "target_sparsity = 0.8\n",
        "total_flops = []\n",
        "for i in range(0,32001,1000):\n",
        "  if i \u003c p_start:\n",
        "    sparsity = 0.\n",
        "  elif p_end \u003c i:\n",
        "    sparsity = target_sparsity\n",
        "  else:\n",
        "    sparsity = (1-(1-(i-p_start)/float(p_end-p_start))**3)*target_sparsity\n",
        "  # print(i, sparsity)\n",
        "  c_flops, _, _ = get_stats(\n",
        "      masked_layers, default_sparsity=sparsity, method='random', custom_sparsities={'conv1/kernel:0':0, 'fc1000/kernel:0':0.8})\n",
        "  # print(i, c_flops, sparsity)\n",
        "  total_flops.append(c_flops)\n",
        "avg_flops = sum(total_flops) / len(total_flops)\n",
        "print('Average Flops: ', avg_flops, avg_flops/total_flops[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUU10hxxsZX-"
      },
      "source": [
        "### Printing sparse network stats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qupDcQOlTxDk"
      },
      "outputs": [],
      "source": [
        "print_stats(masked_layers, 0.8, 'erdos_renyi_kernel', is_debug=True, erk_power_scale=0.2)\n",
        "print_stats(masked_layers, 0.8, 'erdos_renyi')\n",
        "print_stats(masked_layers, 0.8, 'random', {'conv1/kernel:0':0, 'fc1000/kernel:0':0.8}, is_debug=False)\n",
        "print_stats(masked_layers, 0, 'random', is_debug=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI1HIlLrzuED"
      },
      "outputs": [],
      "source": [
        "print_stats(masked_layers, 0.9, 'erdos_renyi_kernel', is_debug=False)\n",
        "print_stats(masked_layers, 0.9, 'erdos_renyi')\n",
        "print_stats(masked_layers, 0.9, 'random', {'conv1/kernel:0':0., 'fc1000/kernel:0':0.9}, is_debug=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX5klsS4_vy-"
      },
      "outputs": [],
      "source": [
        "print_stats(masked_layers, 0.95, 'erdos_renyi_kernel', is_debug=False)\n",
        "print_stats(masked_layers, 0.95, 'erdos_renyi')\n",
        "print_stats(masked_layers, 0.95, 'random', {'conv1/kernel:0':0}, is_debug=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe2FHmPfzS7S"
      },
      "outputs": [],
      "source": [
        "print_stats(masked_layers, 0.965, 'erdos_renyi_kernel', {'conv1/kernel:0':0}, is_debug=False)\n",
        "print_stats(masked_layers, 0.965, 'erdos_renyi')\n",
        "print_stats(masked_layers, 0.965, 'random', {'conv1/kernel:0':0}, is_debug=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc2EeP_YWUfA"
      },
      "source": [
        "## Finding the width Multiplier for small dense model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8NJFEo9Se2S"
      },
      "outputs": [],
      "source": [
        "_, sparse_bits, _ = get_stats(masked_layers, 0.8, 'erdos_renyi_kernel')\n",
        "_, bits, _ = get_stats(masked_layers, 0., 'erdos_renyi_kernel', width=0.465)\n",
        "print(sparse_bits/bits)\n",
        "print_stats(masked_layers, 0., 'erdos_renyi_kernel', is_debug=False, width=0.465)\n",
        "print_stats(masked_layers, 0.8, 'erdos_renyi_kernel', is_debug=False, width=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gjk8Z2g2TOKq"
      },
      "outputs": [],
      "source": [
        "_, sparse_bits, _ = get_stats(masked_layers, 0.9, 'erdos_renyi_kernel')\n",
        "_, bits, _ = get_stats(masked_layers, 0., 'erdos_renyi_kernel', width=0.34)\n",
        "print(sparse_bits/bits)\n",
        "print_stats(masked_layers, 0., 'erdos_renyi_kernel', is_debug=False, width=0.34)\n",
        "print_stats(masked_layers, 0.9, 'erdos_renyi_kernel', is_debug=False, width=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sa1zoC-bT-Qk"
      },
      "outputs": [],
      "source": [
        "_, sparse_bits, _ = get_stats(masked_layers, 0.95, 'erdos_renyi_kernel')\n",
        "_, bits, _ = get_stats(masked_layers, 0., 'erdos_renyi_kernel', width=0.26)\n",
        "print(sparse_bits/bits)\n",
        "print_stats(masked_layers, 0., 'erdos_renyi_kernel', is_debug=False, width=0.26)\n",
        "print_stats(masked_layers, 0.95, 'erdos_renyi_kernel', is_debug=False, width=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_IugJP5URFa"
      },
      "outputs": [],
      "source": [
        "_, sparse_bits, _ = get_stats(masked_layers, 0.965, 'erdos_renyi_kernel')\n",
        "_, bits, _ = get_stats(masked_layers, 0., 'erdos_renyi_kernel', width=0.231)\n",
        "print(sparse_bits/bits)\n",
        "print_stats(masked_layers, 0., 'erdos_renyi_kernel', is_debug=False, width=0.231)\n",
        "print_stats(masked_layers, 0.965, 'erdos_renyi_kernel', is_debug=False, width=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXd4Mx90sc9Q"
      },
      "source": [
        "### Printing the Big-Sparse Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtpJ3LvKYCNn"
      },
      "outputs": [],
      "source": [
        "# BIGGER\n",
        "_, sparse_bits, _ = get_stats(masked_layers, 0., 'erdos_renyi_kernel')\n",
        "_, bits, _ = get_stats(masked_layers, 0.8, 'erdos_renyi_kernel', width=2.1)\n",
        "print(sparse_bits/bits)\n",
        "print_stats(masked_layers, 0.8, 'erdos_renyi_kernel', is_debug=False, width=2.1)\n",
        "print_stats(masked_layers, 0.8, 'random',  {'conv1/kernel:0':0, 'fc1000/kernel:0':0.8},\n",
        "            is_debug=False, width=2.1)\n",
        "print_stats(masked_layers, 0., 'erdos_renyi_kernel', is_debug=False, width=2.1)\n",
        "print_stats(masked_layers, 0., 'erdos_renyi_kernel', is_debug=False, width=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRcOlrf4YG7K"
      },
      "outputs": [],
      "source": [
        "_, sparse_bits, _ = get_stats(masked_layers, 0., 'erdos_renyi_kernel')\n",
        "_, bits, _ = get_stats(masked_layers, 0.9, 'erdos_renyi_kernel', width=2.8)\n",
        "print(sparse_bits/bits)\n",
        "print_stats(masked_layers, 0.9, 'erdos_renyi_kernel', is_debug=False, width=2.8)\n",
        "print_stats(masked_layers, 0.9, 'random',  {'conv1/kernel:0':0, 'fc1000/kernel:0':0.8}, is_debug=False, width=2.8)\n",
        "print_stats(masked_layers, 0., 'erdos_renyi_kernel', is_debug=False, width=2.8)\n",
        "print_stats(masked_layers, 0., 'erdos_renyi_kernel', is_debug=False, width=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN8qfasQWva2"
      },
      "source": [
        "## [BONUS] DSR FLOPs\n",
        "Obtained from figure https://arxiv.org/abs/1902.05967; exact values are probably slightly different.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwI5aRe-SH0n"
      },
      "outputs": [],
      "source": [
        "resnet_layers=['conv1/kernel:0',\n",
        "'res2a_branch2a/kernel:0',\n",
        "'res2a_branch2b/kernel:0',\n",
        "'res2a_branch2c/kernel:0',\n",
        "'res2a_branch1/kernel:0',\n",
        "'res2b_branch2a/kernel:0',\n",
        "'res2b_branch2b/kernel:0',\n",
        "'res2b_branch2c/kernel:0',\n",
        "'res2c_branch2a/kernel:0',\n",
        "'res2c_branch2b/kernel:0',\n",
        "'res2c_branch2c/kernel:0',\n",
        "'res3a_branch2a/kernel:0',\n",
        "'res3a_branch2b/kernel:0',\n",
        "'res3a_branch2c/kernel:0',\n",
        "'res3a_branch1/kernel:0',\n",
        "'res3b_branch2a/kernel:0',\n",
        "'res3b_branch2b/kernel:0',\n",
        "'res3b_branch2c/kernel:0',\n",
        "'res3c_branch2a/kernel:0',\n",
        "'res3c_branch2b/kernel:0',\n",
        "'res3c_branch2c/kernel:0',\n",
        "'res3d_branch2a/kernel:0',\n",
        "'res3d_branch2b/kernel:0',\n",
        "'res3d_branch2c/kernel:0',\n",
        "'res4a_branch2a/kernel:0',\n",
        "'res4a_branch2b/kernel:0',\n",
        "'res4a_branch2c/kernel:0',\n",
        "'res4a_branch1/kernel:0',\n",
        "'res4b_branch2a/kernel:0',\n",
        "'res4b_branch2b/kernel:0',\n",
        "'res4b_branch2c/kernel:0',\n",
        "'res4c_branch2a/kernel:0',\n",
        "'res4c_branch2b/kernel:0',\n",
        "'res4c_branch2c/kernel:0',\n",
        "'res4d_branch2a/kernel:0',\n",
        "'res4d_branch2b/kernel:0',\n",
        "'res4d_branch2c/kernel:0',\n",
        "'res4e_branch2a/kernel:0',\n",
        "'res4e_branch2b/kernel:0',\n",
        "'res4e_branch2c/kernel:0',\n",
        "'res4f_branch2a/kernel:0',\n",
        "'res4f_branch2b/kernel:0',\n",
        "'res4f_branch2c/kernel:0',\n",
        "'res5a_branch2a/kernel:0',\n",
        "'res5a_branch2b/kernel:0',\n",
        "'res5a_branch2c/kernel:0',\n",
        "'res5a_branch1/kernel:0',\n",
        "'res5b_branch2a/kernel:0',\n",
        "'res5b_branch2b/kernel:0',\n",
        "'res5b_branch2c/kernel:0',\n",
        "'res5c_branch2a/kernel:0',\n",
        "'res5c_branch2b/kernel:0',\n",
        "'res5c_branch2c/kernel:0',\n",
        "'fc1000/kernel:0']\n",
        "dsr_sparsities8=[0,\n",
        "            0., .15, .5, .425, .575, .55, .425, .32, .44, .15,\n",
        "            0., .15, .55, .6, .8, .65, .75, .65, .65, .65, .55, .65, .7,\n",
        "            0., .35, .65, .85, .9, .8, .85, .85, .8, .85, .85, .85, .85, .8, .8, .9, .75, .8, .85,\n",
        "            0., .65, .85, .95, .85, .8, .9, .65, .9, .8,\n",
        "            .8]\n",
        "dsr_sparsities9=[0,\n",
        "            0., .4, .6, .65, .65, .6, .6, .5, .6, .45,\n",
        "            0., .4, .7, .8, .9, .8, .85, .8, .75, .8, .7, .8, .8,\n",
        "            0., .6, .8, .95, .95, .9, .95, .9, .9, .95, .9, .9, .95, .9, .9, .95, .85, .85, .9,\n",
        "            0., 0.8, .95, .95, .9, .9, .95, .8, .95, .9,\n",
        "            .9] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6i-jjz6OLBH"
      },
      "outputs": [],
      "source": [
        "dsr_map = dict(zip(resnet_layers, dsr_sparsities8))\n",
        "print_stats(masked_layers, 0., 'random', dsr_map, is_debug=False, width=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeGqdHtYYlZT"
      },
      "outputs": [],
      "source": [
        "dsr_map = dict(zip(resnet_layers, dsr_sparsities9))\n",
        "print_stats(masked_layers, 0., 'random', dsr_map, is_debug=False, width=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3qqLKrG67e"
      },
      "source": [
        "# [BONUS] STR FLOPs\n",
        "Layerwise sparsities are obtained from the [STR paper](https://arxiv.org/abs/2002.03231)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIwBmu0NHOuI"
      },
      "outputs": [],
      "source": [
        "str_sparsities = \"\"\"\n",
        "Layer 1 - conv1 9408 118013952 51.46 51.40 63.02 59.80 59.83 64.87 67.36 66.96 72.11 69.46 73.29 73.47 72.05 75.12 76.12 77.75\n",
        "Layer 2 - layer1.0.conv1 4096 12845056 69.36 73.24 87.57 83.28 85.18 89.60 91.41 91.11 92.38 91.75 94.46 94.51 94.60 95.95 96.53 96.51\n",
        "Layer 3 - layer1.0.conv2 36864 115605504 77.85 76.26 90.87 89.48 87.31 94.79 94.27 95.04 95.69 96.07 97.36 97.77 98.35 98.51 98.59 98.84\n",
        "Layer 4 - layer1.0.conv3 16384 51380224 74.81 74.65 86.52 85.80 85.25 91.85 92.78 93.67 94.13 94.69 96.61 97.03 97.37 98.04 98.21 98.47\n",
        "Layer 5 - layer1.0.downsample.0 16384 51380224 70.95 72.96 83.53 83.34 82.56 89.13 90.62 90.17 91.83 92.69 95.48 94.89 95.68 96.98 97.56 97.72\n",
        "Layer 6 - layer1.1.conv1 16384 51380224 80.27 79.58 89.82 89.89 88.51 94.56 96.64 95.78 95.81 96.81 98.79 98.90 98.98 99.13 99.62 99.47\n",
        "Layer 7 - layer1.1.conv2 36864 115605504 81.36 80.95 91.75 90.60 89.61 94.70 95.78 96.18 96.42 97.26 98.65 99.07 99.40 99.11 99.31 99.56\n",
        "Layer 8 - layer1.1.conv3 16384 51380224 84.45 80.11 91.22 91.70 90.21 95.17 97.05 95.81 96.34 97.23 98.68 98.76 98.90 99.16 99.57 99.46\n",
        "Layer 9 - layer1.2.conv1 16384 51380224 78.23 79.79 90.12 88.07 89.36 94.62 95.94 94.74 96.23 96.75 97.96 98.41 98.72 99.38 99.35 99.46\n",
        "Layer 10 - layer1.2.conv2 36864 115605504 76.01 81.53 91.06 87.03 88.27 93.90 95.63 94.26 96.24 96.11 97.54 98.27 98.44 99.32 99.19 99.39\n",
        "Layer 11 - layer1.2.conv3 16384 51380224 84.47 83.28 94.95 90.99 92.64 95.76 96.95 96.01 96.87 97.31 98.38 98.60 98.72 99.38 99.27 99.51\n",
        "Layer 12 - layer2.0.conv1 32768 102760448 73.74 73.96 86.78 85.95 85.90 92.32 94.79 93.86 94.62 95.64 97.19 98.22 98.52 98.48 98.84 98.92\n",
        "Layer 13 - layer2.0.conv2 147456 115605504 82.56 85.70 91.31 93.91 94.03 97.54 97.43 97.65 98.38 98.62 99.24 99.23 99.40 99.61 99.67 99.63\n",
        "Layer 14 - layer2.0.conv3 65536 51380224 84.70 83.55 93.04 93.13 92.13 96.61 97.37 97.21 97.59 98.14 98.80 98.95 99.18 99.29 99.47 99.43\n",
        "Layer 15 - layer2.0.downsample.0 131072 102760448 85.10 87.66 92.78 94.96 95.13 98.07 97.97 98.15 98.70 98.88 99.37 99.35 99.40 99.69 99.68 99.71\n",
        "Layer 16 - layer2.1.conv1 65536 51380224 85.42 85.79 94.04 95.31 94.94 97.92 98.53 98.21 98.84 99.06 99.46 99.53 99.72 99.78 99.81 99.80\n",
        "Layer 17 - layer2.1.conv2 147456 115605504 76.95 82.75 87.63 91.50 91.76 95.59 97.22 96.07 97.32 97.80 98.24 98.24 98.60 99.24 99.66 99.33\n",
        "Layer 18 - layer2.1.conv3 65536 51380224 84.76 84.71 93.10 93.66 93.23 97.00 98.18 97.35 98.06 98.41 98.96 99.21 99.32 99.55 99.58 99.59\n",
        "Layer 19 - layer2.2.conv1 65536 51380224 84.30 85.34 92.70 94.61 94.76 97.72 97.91 98.21 98.54 98.98 99.24 99.35 99.50 99.62 99.63 99.77\n",
        "Layer 20 - layer2.2.conv2 147456 115605504 84.28 85.43 92.99 94.86 94.90 97.52 97.21 98.11 98.19 99.04 99.28 99.37 99.46 99.63 99.59 99.72\n",
        "Layer 21 - layer2.2.conv3 65536 51380224 82.19 84.21 91.12 93.38 93.53 96.89 97.14 97.59 97.77 98.66 98.96 99.15 99.25 99.49 99.51 99.57\n",
        "Layer 22 - layer2.3.conv1 65536 51380224 83.37 84.41 90.46 93.26 93.50 96.71 97.89 96.99 98.14 98.36 99.10 99.23 99.33 99.53 99.75 99.60\n",
        "Layer 23 - layer2.3.conv2 147456 115605504 82.83 84.03 91.44 93.21 93.25 96.83 98.02 96.96 98.45 98.30 98.97 99.06 99.26 99.31 99.81 99.68\n",
        "Layer 24 - layer2.3.conv3 65536 51380224 82.93 85.65 91.02 94.14 93.56 97.20 97.97 97.04 98.16 98.36 98.88 98.97 99.20 99.32 99.67 99.62\n",
        "Layer 25 - layer3.0.conv1 131072 102760448 76.63 77.98 85.99 88.85 88.60 94.26 95.07 94.97 96.21 96.59 97.75 98.04 98.30 98.72 99.11 99.06\n",
        "Layer 26 - layer3.0.conv2 589824 115605504 87.35 88.68 94.39 96.14 96.19 98.51 98.77 98.72 99.11 99.23 99.53 99.59 99.64 99.73 99.80 99.81\n",
        "Layer 27 - layer3.0.conv3 262144 51380224 81.22 83.22 90.58 93.19 93.05 96.82 97.38 97.32 97.98 98.28 98.88 99.03 99.16 99.39 99.55 99.53\n",
        "Layer 28 - layer3.0.downsample.0 524288 102760448 89.75 90.99 96.05 97.20 97.16 98.96 99.21 99.20 99.50 99.58 99.78 99.82 99.86 99.91 99.94 99.93\n",
        "Layer 29 - layer3.1.conv1 262144 51380224 85.88 87.35 93.43 95.36 96.12 98.64 98.77 98.87 99.22 99.33 99.64 99.67 99.72 99.82 99.88 99.84\n",
        "Layer 30 - layer3.1.conv2 589824 115605504 85.06 86.24 92.74 95.06 95.30 98.09 98.28 98.36 98.75 99.08 99.46 99.48 99.54 99.69 99.76 99.76\n",
        "Layer 31 - layer3.1.conv3 262144 51380224 84.34 86.79 92.15 94.84 94.90 97.75 98.15 98.11 98.56 98.94 99.30 99.36 99.45 99.65 99.79 99.70\n",
        "Layer 32 - layer3.2.conv1 262144 51380224 87.51 89.15 94.15 96.77 96.46 98.81 98.83 98.96 99.19 99.44 99.67 99.71 99.74 99.82 99.85 99.89\n",
        "Layer 33 - layer3.2.conv2 589824 115605504 87.15 88.67 94.09 95.59 96.14 98.86 98.69 98.91 99.21 99.20 99.64 99.72 99.76 99.85 99.84 99.90\n",
        "Layer 34 - layer3.2.conv3 262144 51380224 84.86 86.90 92.40 94.99 94.99 98.19 98.19 98.42 98.76 98.97 99.42 99.56 99.62 99.76 99.75 99.88\n",
        "Layer 35 - layer3.3.conv1 262144 51380224 86.62 89.46 94.06 96.08 95.88 98.70 98.71 98.77 99.01 99.27 99.58 99.66 99.69 99.83 99.87 99.87\n",
        "Layer 36 - layer3.3.conv2 589824 115605504 86.52 87.97 93.56 96.10 96.11 98.70 98.82 98.89 99.19 99.31 99.68 99.73 99.77 99.88 99.87 99.93\n",
        "Layer 37 - layer3.3.conv3 262144 51380224 84.19 86.81 92.32 94.94 94.91 98.20 98.37 98.43 98.82 99.00 99.51 99.57 99.64 99.81 99.81 99.87\n",
        "Layer 38 - layer3.4.conv1 262144 51380224 85.85 88.40 93.55 95.49 95.86 98.35 98.44 98.55 98.79 98.96 99.54 99.59 99.60 99.82 99.86 99.87\n",
        "Layer 39 - layer3.4.conv2 589824 115605504 85.96 87.38 93.27 95.66 95.63 98.41 98.58 98.56 99.19 99.26 99.64 99.69 99.67 99.87 99.90 99.92\n",
        "Layer 40 - layer3.4.conv3 262144 51380224 83.45 85.76 91.75 94.49 94.35 97.67 98.09 97.99 98.65 98.94 99.49 99.52 99.48 99.77 99.86 99.85\n",
        "Layer 41 - layer3.5.conv1 262144 51380224 83.33 85.77 91.79 95.09 94.24 97.46 97.89 97.92 98.71 98.90 99.35 99.52 99.58 99.76 99.79 99.83\n",
        "Layer 42 - layer3.5.conv2 589824 115605504 84.98 86.67 92.48 94.92 95.13 97.88 98.14 98.32 98.91 99.00 99.44 99.58 99.69 99.80 99.83 99.87\n",
        "Layer 43 - layer3.5.conv3 262144 51380224 79.78 82.23 89.39 93.14 92.76 96.59 97.04 97.30 98.10 98.41 99.03 99.25 99.44 99.61 99.71 99.75\n",
        "Layer 44 - layer4.0.conv1 524288 102760448 77.83 79.61 87.11 90.32 90.64 95.39 95.84 95.92 97.17 97.35 98.36 98.60 98.83 99.20 99.37 99.42\n",
        "Layer 45 - layer4.0.conv2 2359296 115605504 86.18 88.00 93.53 95.66 95.78 98.31 98.47 98.55 99.08 99.16 99.54 99.63 99.69 99.81 99.85 99.86\n",
        "Layer 46 - layer4.0.conv3 1048576 51380224 78.43 80.48 87.85 91.14 91.27 96.00 96.40 96.47 97.53 97.92 98.81 99.00 99.15 99.45 99.57 99.61\n",
        "Layer 47 - layer4.0.downsample.0 2097152 102760448 88.49 89.98 95.03 96.79 96.90 98.91 99.06 99.11 99.45 99.51 99.77 99.82 99.85 99.92 99.94 99.94\n",
        "Layer 48 - layer4.1.conv1 1048576 51380224 82.07 84.02 90.34 93.69 93.72 97.15 97.56 97.76 98.45 98.75 99.27 99.36 99.54 99.67 99.76 99.80\n",
        "Layer 49 - layer4.1.conv2 2359296 115605504 83.42 85.23 91.16 93.98 93.93 97.26 97.58 97.71 98.36 98.67 99.25 99.34 99.50 99.68 99.76 99.80\n",
        "Layer 50 - layer4.1.conv3 1048576 51380224 78.08 79.96 86.66 90.48 90.22 95.22 95.76 95.89 96.88 97.65 98.70 98.85 99.13 99.45 99.58 99.66\n",
        "Layer 51 - layer4.2.conv1 1048576 51380224 76.34 77.93 84.98 87.57 88.47 93.90 93.87 94.16 95.55 95.91 97.66 97.97 98.15 98.88 99.08 99.22\n",
        "Layer 52 - layer4.2.conv2 2359296 115605504 73.57 74.97 82.32 84.37 86.01 91.92 91.66 92.22 94.02 94.16 96.65 97.13 97.29 98.44 98.74 99.00\n",
        "Layer 53 - layer4.2.conv3 1048576 51380224 68.78 70.38 78.11 80.29 81.73 89.64 89.43 89.65 91.40 92.65 96.02 96.72 96.93 98.47 98.83 99.15\n",
        "Layer 54 - fc 2048000 2048000 50.65 52.46 60.48 64.50 65.12 75.20 75.73 75.80 78.57 80.69 85.96 87.26 88.03 91.11 92.15 92.87\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSFw1eH1G8zh"
      },
      "outputs": [],
      "source": [
        "resnet_layers=['conv1/kernel:0',\n",
        "'res2a_branch2a/kernel:0',\n",
        "'res2a_branch2b/kernel:0',\n",
        "'res2a_branch2c/kernel:0',\n",
        "'res2a_branch1/kernel:0',\n",
        "'res2b_branch2a/kernel:0',\n",
        "'res2b_branch2b/kernel:0',\n",
        "'res2b_branch2c/kernel:0',\n",
        "'res2c_branch2a/kernel:0',\n",
        "'res2c_branch2b/kernel:0',\n",
        "'res2c_branch2c/kernel:0',\n",
        "'res3a_branch2a/kernel:0',\n",
        "'res3a_branch2b/kernel:0',\n",
        "'res3a_branch2c/kernel:0',\n",
        "'res3a_branch1/kernel:0',\n",
        "'res3b_branch2a/kernel:0',\n",
        "'res3b_branch2b/kernel:0',\n",
        "'res3b_branch2c/kernel:0',\n",
        "'res3c_branch2a/kernel:0',\n",
        "'res3c_branch2b/kernel:0',\n",
        "'res3c_branch2c/kernel:0',\n",
        "'res3d_branch2a/kernel:0',\n",
        "'res3d_branch2b/kernel:0',\n",
        "'res3d_branch2c/kernel:0',\n",
        "'res4a_branch2a/kernel:0',\n",
        "'res4a_branch2b/kernel:0',\n",
        "'res4a_branch2c/kernel:0',\n",
        "'res4a_branch1/kernel:0',\n",
        "'res4b_branch2a/kernel:0',\n",
        "'res4b_branch2b/kernel:0',\n",
        "'res4b_branch2c/kernel:0',\n",
        "'res4c_branch2a/kernel:0',\n",
        "'res4c_branch2b/kernel:0',\n",
        "'res4c_branch2c/kernel:0',\n",
        "'res4d_branch2a/kernel:0',\n",
        "'res4d_branch2b/kernel:0',\n",
        "'res4d_branch2c/kernel:0',\n",
        "'res4e_branch2a/kernel:0',\n",
        "'res4e_branch2b/kernel:0',\n",
        "'res4e_branch2c/kernel:0',\n",
        "'res4f_branch2a/kernel:0',\n",
        "'res4f_branch2b/kernel:0',\n",
        "'res4f_branch2c/kernel:0',\n",
        "'res5a_branch2a/kernel:0',\n",
        "'res5a_branch2b/kernel:0',\n",
        "'res5a_branch2c/kernel:0',\n",
        "'res5a_branch1/kernel:0',\n",
        "'res5b_branch2a/kernel:0',\n",
        "'res5b_branch2b/kernel:0',\n",
        "'res5b_branch2c/kernel:0',\n",
        "'res5c_branch2a/kernel:0',\n",
        "'res5c_branch2b/kernel:0',\n",
        "'res5c_branch2c/kernel:0',\n",
        "'fc1000/kernel:0']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31sg-lNhHN7D"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "str_sparsities_parsed = defaultdict(list)\n",
        "for j, l in enumerate(str_sparsities.strip().split('\\n')):\n",
        "  l = l.split('-')[1].strip().split(' ')\n",
        "  if l[0] == 'Overall':\n",
        "    overall_sparsities = map(float, l[3:])\n",
        "  else:\n",
        "    for i, ls in enumerate(l[3:]):\n",
        "      s = overall_sparsities[i]\n",
        "      # Accuracies are between 0 and 1, so devide by 100.\n",
        "      str_sparsities_parsed[s].append(float(ls) / 100.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrjtum-4HgAT"
      },
      "outputs": [],
      "source": [
        "for k in str_sparsities_parsed:\n",
        "  print(k)\n",
        "  dsr_map = dict(zip(resnet_layers, str_sparsities_parsed[k]))\n",
        "  print_stats(masked_layers, 0., 'random', dsr_map, is_debug=False, width=1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//research/colab/notebook:notebook_backend",
        "kind": "private"
      },
      "name": "Resnet-50: Param/Flops Counting [OpenSource].ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
